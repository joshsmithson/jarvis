### Product Document: Real-Time Jarvis Voice Chatbot

Based on your vision, here is a product document outlining the app's goals, technical specifications, user experience, and future considerations.

---

### 1. Product Vision

A minimalist, personal web application that provides a real-time, voice-based conversational experience with a "Jarvis-like" AI. The core of the experience is a natural, low-latency "call" with the AI, where users can have a general conversation. Conversations are saved for later review, with a clear path for expanding the AI's capabilities to perform real-world actions.

---

### 2. Technology Stack

* **Frontend:** Next.js with React
    * **UI Framework:** MUI (Material UI) with a focus on responsive, mobile-first components.
    * **Styling:** Custom, minimalist styling to ensure a clean, futuristic look.
* **Backend & Database:** Supabase
    * **Authentication:** Supabase Auth for Google Sign-in.
    * **Database:** Supabase Postgres for storing user profiles and conversation history.
    * **API Routes:** Next.js API Routes to securely handle communication with third-party services like ElevenLabs and OpenAI.
* **AI Services:**
    * **Speech-to-Text (STT) & Text-to-Speech (TTS):** ElevenLabs Conversational AI API. This will be the primary engine for real-time, low-latency voice communication.
    * **Large Language Model (LLM):** GPT-5 for the initial conversational model, with the flexibility to swap out models for cost optimization.

---

### 3. User Experience (UX) & Interface (UI) Choices

#### 3.1. User Flow

1.  **Sign-in:** User lands on the app's homepage and signs in using a "Sign in with Google" button, powered by Supabase Auth.
2.  **Home Screen:** After successful sign-in, the user is taken to a simple home screen with a list of past conversations and a prominent button (e.g., a phone icon) to "Start a new Call with Jarvis."
3.  **The "Call" Experience:**
    * Clicking the "Start Call" button navigates the user to a new screen.
    * The screen will feature a large, central "End Call" button.
    * During the conversation, a visual cue (e.g., a simple sound wave animation or a pulsing icon) will indicate that the AI is listening, processing, or speaking.
4.  **Ending the Conversation:**
    * The user presses the "End Call" button.
    * A modal or notification will appear, asking, "Would you like to save this conversation?"
    * The user can choose to save the conversation to their history or discard it.
5.  **Conversation History:**
    * The home screen will display a list of saved conversations.
    * Each list item will show the date and a brief summary of the conversation.
    * Clicking a conversation item will open a new view, displaying the full text transcript.
    * Within this view, the user can replay the audio (generated by ElevenLabs) or delete the entire conversation. Editing will not be an option.

#### 3.2. UI Design

* **Aesthetics:** Clean, minimalist, and mobile-first. The design will prioritize clarity and ease of use, avoiding heavy theming or visual clutter.
* **Components:** The application will use MUI components to maintain consistency and a professional look across all devices.

---

### 4. Technical Implementation & Architecture

#### 4.1. Real-Time Latency (Sub-1 Second)

To achieve a natural, sub-1-second response time, the system will use a streaming architecture:
* The web app will capture the user's voice and stream it in real-time to the ElevenLabs API for Speech-to-Text (STT).
* As the first few words are transcribed, the partial text will be immediately sent to the LLM (GPT-5).
* As the LLM starts generating its response, the first few tokens of text will be streamed back to the ElevenLabs API for Text-to-Speech (TTS).
* The audio from the TTS will be streamed back to the user's browser, allowing Jarvis to begin speaking even before the user has finished their sentence, or the full LLM response has been generated. This "interruptibility" is key to a fluid conversation.

#### 4.2. API & Data Handling

* **Secure API Routes:** All sensitive API keys (e.g., ElevenLabs and OpenAI) will be stored on the server side using Next.js API Routes. The client will never have direct access to these keys.
* **Database Schema (Supabase):** The Supabase database will have at least two tables:
    * `users`: Stores user data, linked to the `auth.users` table for authenticated access.
    * `conversations`: Stores conversation history, including a `user_id` foreign key, a `timestamp`, the full text transcript, and a link to the generated audio file (if saved).
* **Storage:** Supabase Storage can be used to store the generated audio files for later replay.

#### 4.3. Future-Proofing for "Actionable AI"

The architecture will be designed to support the addition of "tools" in the future. The LLM can be configured to use a function-calling framework to parse user requests. For example, if the user says, "Jarvis, add an event to my calendar," the LLM will recognize this as a request for a `createCalendarEvent` function, which can be handled by a separate Next.js API route that interacts with a third-party calendar API.

#### 4.4. Error Handling

* A dedicated UI component will display clear, user-friendly error messages (e.g., "Microphone access denied," "Connection lost, please check your internet," or "Jarvis is currently unavailable, please try again later").
* The user will be given a clear action to take, such as a "Retry" button.

This document serves as a blueprint for development, ensuring all team members are aligned on the product's vision and technical direction.